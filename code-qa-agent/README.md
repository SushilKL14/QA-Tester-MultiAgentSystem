ğŸš€ Code-QA-Agent
A Multi-Agent Automated System for Code Understanding, Test Generation, Test Execution, and Bug Reporting
This project implements a multi-agent architecture that analyzes Python files, generates tests, runs them in isolation, and produces structured bug reports.
The system exposes a Gradio interface for easy interaction and demonstration.

ğŸ” 1. Problem the Project Solves
Manually reviewing Python code is slow and inconsistent. Developers need:
â€¢	Automated understanding of unfamiliar code.
â€¢	Automatic generation of meaningful test cases.
â€¢	On-the-fly execution of tests.
â€¢	Clear, structured reports summarizing issues.
This project solves that by using four cooperative agents that process any uploaded Python file end-to-end.

ğŸ§  2. Solution Summary
The system contains a multi-agent pipeline:
Agent 1: Code Understanding Agent
Analyzes the file and extracts:
â€¢	Functions
â€¢	Parameters
â€¢	Expected behaviors
â€¢	Logical flow
â€¢	Potential edge cases
Agent 2: Test Generator Agent
Uses the understanding from Agent 1 to generate:
â€¢	Unit tests (PyTest)
â€¢	Boundary tests
â€¢	Negative tests
â€¢	Edge-case scenarios
Agent 3: Test Runner Agent
Executes the generated tests inside a safe temporary workspace:
â€¢	Captures stdout / stderr
â€¢	Detects failures
â€¢	Sanitizes noise from PyTest output
Agent 4: Bug Reporter Agent
Creates a clean report:
â€¢	Summary
â€¢	What passed / failed
â€¢	Potential root cause
â€¢	Suggestions for fixes
Everything flows through src/pipeline.py, which orchestrates the agents.














ğŸ— 3. Project Structure
code-qa-agent/
â”œâ”€ data/
â”‚  â”œâ”€ samples/                 # sample repos / code files for demo
â”œâ”€ notebooks/
â”‚  â”œâ”€ demo_notebook.ipynb
â”œâ”€ src/
â”‚  â”œâ”€ agents/
â”‚  â”‚  â”œâ”€ code_understanding.py
â”‚  â”‚  â”œâ”€ test_generator.py
â”‚  â”‚  â”œâ”€ test_runner.py
â”‚  â”‚  â””â”€ bug_reporter.py
â”‚  â”œâ”€ tools/
â”‚  â”‚  â”œâ”€ file_utils.py
â”‚  â”‚  â”œâ”€ session_memory.py
â”‚  â”‚  â””â”€ observability.py
â”‚  â”œâ”€ pipeline.py
â”œâ”€ demo/
â”‚  â”œâ”€ streamlit_app.py
â”œâ”€ tests/                      # unit tests for the agent code itself
â”œâ”€ .github/
â”‚  â”œâ”€ workflows/ci.yaml        # optional CI example
â”œâ”€ README.md
â”œâ”€ requirements.txt
â””â”€ writeup.md
ğŸ› 4. Demo Application (Gradio)
The UI is built in demo/app.py with the following features:
âœ” Upload any .py file
âœ” Pipeline runs automatically
âœ” Pretty-formatted output
âœ” Handles PyTest noise / long outputs
âœ” Detects failures and missing tests
âœ” Shows final agent result clearly
This is your actual logic:
â€¢	safe_run() handles:
o	file saving
o	pipeline execution
o	formatting
o	crash protection
â€¢	format_pretty_output() cleans PyTest noise
â€¢	Output is shown via a large textbox
To launch:
python demo/app.py
Kaggle automatically forces share=True, so the UI will get a public link.

âš™ 5. Installation
1. Clone repo
git clone https://github.com/your-username/code-qa-agent.git
cd code-qa-agent
2. Install dependencies
pip install -r requirements.txt
3. Run the demo
python demo/app.py

ğŸ§ª 6. How the Multi-Agent Pipeline Works
Step-by-Step Execution Flow
1.	User uploads a file â†’ app.py saves it
2.	pipeline.py reads the file
3.	Code Understanding Agent extracts structure
4.	Test Generator Agent creates tests
5.	Test Runner Agent runs PyTest safely
6.	Bug Reporter Agent summarizes all results
7.	UI displays the cleaned final output

ğŸ“Œ 7. Example Output (Realistic)
âœ” All tests passed

Generated Tests:
- test_add_positive_numbers
- test_add_negative_numbers
- test_add_zero

Execution Report:
3 passed, 0 failed in 0.01s
Or a failed case:
âŒ Tests failed

FAILED test_file.py::test_divide_by_zero
ZeroDivisionError: division by zero

Suggested Fix:
Add input validation for divisor == 0

ğŸ“š 8. Key Concepts Used (for scoring)
Your project clearly demonstrates:
âœ” Multi-Agent Architecture
(distinct agents with separate responsibilities)
âœ” Memory & Session Tracking
(session_memory.py maintains persistent pipeline state)
âœ” Observability
(custom logs + execution traces)
âœ” Code Understanding & Reasoning
(the understanding agent analyzes AST-level information)
âœ” Automated Test Generation
(logic-driven PyTest file creation)
âœ” Sandbox Test Execution
(runs tests inside temporary directories)
âœ” Front-end Integration (Gradio)
(interactive uploader + output formatting)
This checks all requirements for the "Implementation" category.

ğŸ§¾ 9. File: writeup.md
You should summarize:
â€¢	architecture
â€¢	decisions
â€¢	agent roles
â€¢	pipeline flow
â€¢	screenshots of UI
â€¢	example output
Keep it concise, technical, and architecture-driven.

ğŸ” 10. Security Note
This project never stores API keys, and is safe for evaluation.

ğŸ 11. Conclusion
This project demonstrates:
â€¢	solid multi-agent design
â€¢	automated code reasoning
â€¢	test generation
â€¢	reliable test execution
â€¢	clean UI
â€¢	strong engineering structure
Everything is modular, testable, and easy to extend.

